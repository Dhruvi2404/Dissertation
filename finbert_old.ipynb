{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ku1hgShjPfBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers"
      ],
      "metadata": {
        "id": "mvvKiuZbPg14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Zr2ZWIxLQw-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_news_df = pd.read_csv('/content/drive/MyDrive/final_news_df_fin.csv')"
      ],
      "metadata": {
        "id": "xV7ij0zGPOoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_news_df.clean_headline = final_news_df.clean_headline.fillna('')"
      ],
      "metadata": {
        "id": "TH6PggFmQ7xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK_af_y7O-QQ",
        "outputId": "ab57d219-b572-4071-dfeb-3601ada77028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at yiyanghkust/finbert-tone were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at yiyanghkust/finbert-tone.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "358/358 [==============================] - 215s 571ms/step\n"
          ]
        }
      ],
      "source": [
        "# Load the saved model\n",
        "model_dir = 'yiyanghkust/finbert-tone'\n",
        "model = TFBertForSequenceClassification.from_pretrained(model_dir, num_labels=3,ignore_mismatched_sizes=True)\n",
        "\n",
        "df = final_news_df\n",
        "\n",
        "# Tokenize the text using BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenized_texts = []\n",
        "for _, row in df.iterrows():\n",
        "    tokenized_text = tokenizer.tokenize(row[df.columns[4]]) # change the \"df.columns[1]\" to whatver your column number is.\n",
        "    tokenized_texts.append(tokenized_text)\n",
        "\n",
        "# Convert text to input features compatible with BERT\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for tokens in tokenized_texts:\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        ' '.join(tokens),  # Join the tokens into a string\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    input_ids.append(inputs['input_ids'])\n",
        "    attention_masks.append(inputs['attention_mask'])\n",
        "\n",
        "input_ids = tf.concat(input_ids, axis=0)\n",
        "attention_masks = tf.concat(attention_masks, axis=0)\n",
        "\n",
        "# Make predictions using the saved model\n",
        "input_ids = input_ids.numpy()\n",
        "attention_masks = attention_masks.numpy()\n",
        "outputs = model.predict([input_ids, attention_masks])\n",
        "predictions = np.argmax(outputs[0], axis=1)\n",
        "# scores = np.argmax(outputs[1], axis=1)\n",
        "\n",
        "# Add the predictions to the dataframe\n",
        "final_news_df['predictions']= predictions # the predicted sentiments will be stored in a new column named \"my_sentiment\"\n",
        "# final_news_df['pred_scores']= scores\n",
        "# Save the dataframe as a CSV file\n",
        "# output_file = \"InSamp_final_finbert.csv\"\n",
        "# final_news_df.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5Glli8sUXa6",
        "outputId": "65aabf1c-e149-4082-a038-ec8cca82d899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFSequenceClassifierOutput(loss=None, logits=array([[-5.693507 , 10.599956 , -5.6275463],\n",
              "       [-2.7757354,  6.4353065, -4.3515368],\n",
              "       [-4.817431 ,  9.611035 , -5.2125845],\n",
              "       ...,\n",
              "       [-5.5682235, 10.836731 , -5.857997 ],\n",
              "       [-4.451083 ,  9.318366 , -5.2299595],\n",
              "       [-5.6892686, 10.695096 , -5.294671 ]], dtype=float32), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy"
      ],
      "metadata": {
        "id": "7BHMxjVeVMRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = scipy.special.softmax(np.array(outputs[0]))\n",
        "scores = pd.Series(logits[:, 0] - logits[:, 1])"
      ],
      "metadata": {
        "id": "lhobzBApUsOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_news_df['pred_scores']= scores\n",
        "# output_file = \"InSamp_final_finbert.csv\"\n",
        "final_news_df.to_csv(\"InSamp_final_finbert.csv\")"
      ],
      "metadata": {
        "id": "Y8hkPCD9VaBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "VcMU1mVZWQut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_news_df.sent_score = final_news_df.sent_score.fillna(0)"
      ],
      "metadata": {
        "id": "SF6HRY__W3pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"accuracy score: \", accuracy_score(final_news_df['sent_score'],predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkYfbuN1RmW0",
        "outputId": "97bd785d-fd4b-4380-d3be-5d94d1cc1e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy score:  0.30330304089479204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_news_df_out = pd.read_csv('/content/drive/MyDrive/final_news_df_out_fin.csv')"
      ],
      "metadata": {
        "id": "bs0SRW2hakdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_news_df_out.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnBwWVdXcA7x",
        "outputId": "c43b61af-2202-4b21-9bcd-5a336ca5c9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'week', 'FROM', 'TO', 'headline', 'clean_headline',\n",
              "       'returns', 'tf-idf', 'sentiment', 'sent_score', 'compname'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_news_df_out.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "E7YTuOWgZO71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #predicting\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "# import tensorflow as tf\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "final_news_df_out = pd.read_csv('/content/drive/MyDrive/final_news_df_out_fin.csv')\n",
        "final_news_df_out.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "final_news_df_out.clean_headline = final_news_df_out.clean_headline.fillna('')\n",
        "df_out = final_news_df_out\n",
        "\n",
        "# Tokenize the text using BERT tokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenized_texts = []\n",
        "for _, row in df_out.iterrows():\n",
        "    # print(row[df_out.columns[4]])\n",
        "    tokenized_text = tokenizer.tokenize(row[df_out.columns[4]]) # change the \"df.columns[1]\" to whatver your column number is.\n",
        "    tokenized_texts.append(tokenized_text)\n",
        "\n",
        "# Convert text to input features compatible with BERT\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for tokens in tokenized_texts:\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        ' '.join(tokens),  # Join the tokens into a string\n",
        "        add_special_tokens=True,\n",
        "        max_length=256,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    input_ids.append(inputs['input_ids'])\n",
        "    attention_masks.append(inputs['attention_mask'])\n",
        "\n",
        "input_ids = tf.concat(input_ids, axis=0)\n",
        "attention_masks = tf.concat(attention_masks, axis=0)\n",
        "\n",
        "# Make predictions using the saved model\n",
        "input_ids = input_ids.numpy()\n",
        "attention_masks = attention_masks.numpy()\n",
        "outs = model.predict([input_ids, attention_masks])\n",
        "predictions_out = np.argmax(outs[0], axis=1)\n",
        "logits_out = scipy.special.softmax(np.array(outs[0]))\n",
        "scores_out = pd.Series(logits_out[:, 0] - logits_out[:, 1])\n",
        "# Add the predictions to the dataframe\n",
        "final_news_df_out['predictions']= predictions_out\n",
        "final_news_df_out['pred_scores']= scores_out\n",
        "\n",
        "# output_file = \"OutSamp_final_finbert.csv\"\n",
        "final_news_df.to_csv(\"OutSamp_final_finbert.csv\", index=False)\n",
        "\n",
        "print(\"accuracy score: \", accuracy_score(final_news_df_out['sent_score'],predictions_out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jhl_3OmaRIMG",
        "outputId": "ba49d324-42c2-48b5-f542-a48f5bfe0c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227/227 [==============================] - 130s 574ms/step\n",
            "accuracy score:  0.3560606060606061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ArUCK61ZtUP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}